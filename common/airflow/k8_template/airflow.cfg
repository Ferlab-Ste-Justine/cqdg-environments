[core]
executor=KubernetesExecutor
sql_alchemy_conn=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow

[kubernetes]
pod_template_file =
worker_container_repository = apache/airflow
worker_container_tag =1.10.14-python3.7
worker_container_image_pull_policy = IfNotPresent
delete_worker_pods = True
delete_worker_pods_on_failure = False
worker_pods_creation_batch_size = 1
namespace = airflow
multi_namespace_mode = True
airflow_configmap = 
airflow_local_settings_configmap =
dags_in_image = True
dags_volume_subpath = /opt/airflow
dags_volume_mount_point = /opt/airflow
dags_volume_claim =
logs_volume_subpath =
logs_volume_claim =
dags_volume_host = /opt/airflow
logs_volume_host =

# A list of configMapsRefs to envFrom. If more than one configMap is
# specified, provide a comma separated list: configmap_a,configmap_b
env_from_configmap_ref = airflow-config-core,airflow-config-secrets,airflow-config-cli,airflow-config-api,airflow-config-operators,airflow-config-webserver,airflow-config-email,airflow-config-scheduler

# A list of secretRefs to envFrom. If more than one secret is
# specified, provide a comma separated list: secret_a,secret_b
env_from_secret_ref = airflow-smtp

# Git credentials and repository for DAGs mounted via Git (mutually exclusive with volume claim)
git_repo =
git_branch =

# Use a shallow clone with a history truncated to the specified number of commits.
# 0 - do not use shallow clone.
git_sync_depth = 1
git_subpath =

# The specific rev or hash the git_sync init container will checkout
# This becomes GIT_SYNC_REV environment variable in the git_sync init container for worker pods
git_sync_rev =

# Use git_user and git_password for user authentication or git_ssh_key_secret_name
# and git_ssh_key_secret_key for SSH authentication
git_user =
git_password =
git_sync_root = /git
git_sync_dest = repo

# Mount point of the volume if git-sync is being used.
# i.e. /opt/airflow/dags
git_dags_folder_mount_point =

# To get Git-sync SSH authentication set up follow this format
#
# ``airflow-secrets.yaml``:
#
# .. code-block:: yaml
#
#   ---
#   apiVersion: v1
#   kind: Secret
#   metadata:
#     name: airflow-secrets
#   data:
#     # key needs to be gitSshKey
#     gitSshKey: <base64_encoded_data>
# Example: git_ssh_key_secret_name = airflow-secrets
git_ssh_key_secret_name =

# To get Git-sync SSH authentication set up follow this format
#
# ``airflow-configmap.yaml``:
#
# .. code-block:: yaml
#
#   ---
#   apiVersion: v1
#   kind: ConfigMap
#   metadata:
#     name: airflow-configmap
#   data:
#     known_hosts: |
#         github.com ssh-rsa <...>
#     airflow.cfg: |
#         ...
# Example: git_ssh_known_hosts_configmap_name = airflow-configmap
git_ssh_known_hosts_configmap_name =

# To give the git_sync init container credentials via a secret, create a secret
# with two fields: GIT_SYNC_USERNAME and GIT_SYNC_PASSWORD (example below) and
# add ``git_sync_credentials_secret = <secret_name>`` to your airflow config under the
# ``kubernetes`` section
#
# Secret Example:
#
# .. code-block:: yaml
#
#   ---
#   apiVersion: v1
#   kind: Secret
#   metadata:
#     name: git-credentials
#   data:
#     GIT_SYNC_USERNAME: <base64_encoded_git_username>
#     GIT_SYNC_PASSWORD: <base64_encoded_git_password>
git_sync_credentials_secret =

# For cloning DAGs from git repositories into volumes: https://github.com/kubernetes/git-sync
git_sync_container_repository = k8s.gcr.io/git-sync
git_sync_container_tag = v3.1.1
git_sync_init_container_name = git-sync-clone
git_sync_run_as_user = 65533

# The name of the Kubernetes service account to be associated with airflow workers, if any.
# Service accounts are required for workers that require access to secrets or cluster resources.
# See the Kubernetes RBAC documentation for more:
# https://kubernetes.io/docs/admin/authorization/rbac/
worker_service_account_name =

# Any image pull secrets to be given to worker pods, If more than one secret is
# required, provide a comma separated list: secret_a,secret_b
image_pull_secrets =

# GCP Service Account Keys to be provided to tasks run on Kubernetes Executors
# Should be supplied in the format: key-name-1:key-path-1,key-name-2:key-path-2
gcp_service_account_keys =

# Use the service account kubernetes gives to pods to connect to kubernetes cluster.
# It's intended for clients that expect to be running inside a pod running on kubernetes.
# It will raise an exception if called from a process not running in a kubernetes environment.
in_cluster = True

# When running with in_cluster=False change the default cluster_context or config_file
# options to Kubernetes client. Leave blank these to use default behaviour like ``kubectl`` has.
# cluster_context =
# config_file =

# Affinity configuration as a single line formatted JSON object.
# See the affinity model for top-level key names (e.g. ``nodeAffinity``, etc.):
# https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.12/#affinity-v1-core
affinity =

# A list of toleration objects as a single line formatted JSON array
# See:
# https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.12/#toleration-v1-core
tolerations =

# Keyword parameters to pass while calling a kubernetes client core_v1_api methods
# from Kubernetes Executor provided as a single line formatted JSON dictionary string.
# List of supported params are similar for all core_v1_apis, hence a single config
# variable for all apis.
# See:
# https://raw.githubusercontent.com/kubernetes-client/python/master/kubernetes/client/apis/core_v1_api.py
# Note that if no _request_timeout is specified, the kubernetes client will wait indefinitely
# for kubernetes api responses, which will cause the scheduler to hang.
# The timeout is specified as [connect timeout, read timeout]
kube_client_request_args =

# Optional keyword arguments to pass to the ``delete_namespaced_pod`` kubernetes client
# ``core_v1_api`` method when using the Kubernetes Executor.
# This should be an object and can contain any of the options listed in the ``v1DeleteOptions``
# class defined here:
# https://github.com/kubernetes-client/python/blob/41f11a09995efcd0142e25946adc7591431bfb2f/kubernetes/client/models/v1_delete_options.py#L19
# Example: delete_option_kwargs = {"grace_period_seconds": 10}
delete_option_kwargs =

# Specifies the uid to run the first process of the worker pods containers as
run_as_user = 50000

# Specifies a gid to associate with all containers in the worker pods
# if using a git_ssh_key_secret_name use an fs_group
# that allows for the key to be read, e.g. 65533
fs_group =

[kubernetes_node_selectors]

# The Key-value pairs to be given to worker pods.
# The worker pods will be scheduled to the nodes of the specified key-value pairs.
# Should be supplied in the format: key = value

[kubernetes_annotations]

# The Key-value annotations pairs to be given to worker pods.
# Should be supplied in the format: key = value

[kubernetes_environment_variables]

# The scheduler sets the following environment variables into your workers. You may define as
# many environment variables as needed and the kubernetes launcher will set them in the launched workers.
# Environment variables in this section are defined as follows
# ``<environment_variable_key> = <environment_variable_value>``
#
# For example if you wanted to set an environment variable with value `prod` and key
# ``ENVIRONMENT`` you would follow the following format:
# ENVIRONMENT = prod
#
# Additionally you may override worker airflow settings with the ``AIRFLOW__<SECTION>__<KEY>``
# formatting as supported by airflow normally.

[kubernetes_secrets]

# The scheduler mounts the following secrets into your workers as they are launched by the
# scheduler. You may define as many secrets as needed and the kubernetes launcher will parse the
# defined secrets and mount them as secret environment variables in the launched workers.
# Secrets in this section are defined as follows
# ``<environment_variable_mount> = <kubernetes_secret_object>=<kubernetes_secret_key>``
#
# For example if you wanted to mount a kubernetes secret key named ``postgres_password`` from the
# kubernetes secret object ``airflow-secret`` as the environment variable ``POSTGRES_PASSWORD`` into
# your workers you would follow the following format:
# ``POSTGRES_PASSWORD = airflow-secret=postgres_credentials``
#
# Additionally you may override worker airflow settings with the ``AIRFLOW__<SECTION>__<KEY>``
# formatting as supported by airflow normally.

[kubernetes_labels]

# The Key-value pairs to be given to worker pods.
# The worker pods will be given these static labels, as well as some additional dynamic labels
# to identify the task.
# Should be supplied in the format: ``key = value``